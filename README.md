# Benchmarking Generative AI in EDA Workflows

A comprehensive benchmarking framework for evaluating open-source generative AI models on automated Verilog HDL and testbench generation tasks.

## ğŸ“‹ Overview

This project establishes the first structured, reproducible benchmark for AI-assisted hardware design at the RTL (Register Transfer Level). It evaluates models across:

- **Circuit (HDL) generation** from textual specifications
- **Testbench generation** for functional verification
- **Quantitative benchmarking** against reference implementations

## ğŸ¯ Models Evaluated

| Tier | Model | Size | Purpose |
|------|-------|------|---------|
| Large | Llama 3 8B Instruct | 8B | High-quality general-purpose baseline |
| Medium | StarCoder2 | 7B | Code-specialized mid-tier model |
| Small | TinyLlama | 1.1B | Lightweight resource-constrained baseline |

## ğŸ“Š Evaluation Metrics

### Primary Metrics
- **Syntax Validity (SV)**: % of files that compile without errors
- **Functional Correctness (FC)**: % producing expected simulation outputs
- **Synthesis Quality (SQ)**: Cell count and logic depth
- **Testbench Detection Rate (TDR)**: Fault detection capability
- **Generation Time (GT)**: Average inference time

### Secondary Metrics
- **Prompt Sensitivity (PS)**: Variance across prompt templates
- **Hallucination Index (HI)**: Invalid construct frequency
- **Usability Score (US)**: Composite quality metric

## ğŸš€ Quick Start

### Prerequisites

**Required Tools:**
```bash
# Ubuntu/Debian
sudo apt-get install iverilog verilator yosys python3.10 python3-pip

# macOS
brew install icarus-verilog verilator yosys python@3.10
```

**Python Dependencies:**
```bash
pip install -r requirements.txt
```

**AI Models (Ollama):**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull models
ollama pull llama3
ollama pull tinyllama
```

### Running Your First Benchmark

1. **Validate Dataset:**
```bash
cd Quantitative
python dataset_loader.py
```

2. **Run Mini Benchmark:**
```bash
python run_mini_benchmark.py
```

This will evaluate 5 tasks with available models and save results to `../results/mini_benchmark/`.

3. **Analyze Results:**
```bash
python statistical_analysis.py ../results/mini_benchmark/benchmark_results.json
```

4. **Generate Visualizations:**
```bash
python visualizations.py ../results/mini_benchmark/benchmark_results.json ../figures/
```

## ğŸ³ Docker Setup

### Build and Run with Docker:

```bash
# Build image
docker-compose build

# Run container
docker-compose up -d

# Access shell
docker exec -it eda_benchmark bash

# Inside container:
cd /workspace/Quantitative
python run_mini_benchmark.py
```

## ğŸ“ Project Structure

```
Paper_Own/
â”œâ”€â”€ Quantitative/
â”‚   â”œâ”€â”€ Eval_Pipeline.py          # Main evaluation pipeline
â”‚   â”œâ”€â”€ model_interface.py        # AI model integration (Ollama/HF)
â”‚   â”œâ”€â”€ dataset_loader.py         # Task loading utilities
â”‚   â”œâ”€â”€ statistical_analysis.py   # Statistical analysis module
â”‚   â”œâ”€â”€ visualizations.py         # Plotting and visualization
â”‚   â”œâ”€â”€ run_mini_benchmark.py     # Quick test runner
â”‚   â””â”€â”€ dataset/
â”‚       â”œâ”€â”€ tasks.json            # Task metadata
â”‚       â”œâ”€â”€ combinational/        # Combinational circuits
â”‚       â”œâ”€â”€ sequential/           # Sequential circuits
â”‚       â”œâ”€â”€ fsm/                  # Finite state machines
â”‚       â””â”€â”€ mixed/                # Mixed designs
â”œâ”€â”€ results/                      # Generated outputs
â”œâ”€â”€ figures/                      # Visualization outputs
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Container definition
â””â”€â”€ README.md                     # This file
```

## ğŸ“– Usage Examples

### Test a Single Model:
```python
from model_interface import OllamaInterface
from dataset_loader import load_tasks_from_json
from Eval_Pipeline import BenchmarkPipeline

# Load tasks
tasks = load_tasks_from_json("dataset/tasks.json")

# Initialize model
model = OllamaInterface("llama3")

# Run evaluation
pipeline = BenchmarkPipeline(Path("./results"))
metrics = pipeline.evaluate_task(tasks[0], model)
```

### Compare Models:
```python
from statistical_analysis import BenchmarkAnalyzer

analyzer = BenchmarkAnalyzer("results/benchmark_results.json")
analyzer.print_summary_report()

# Statistical test
result = analyzer.paired_statistical_test("Llama-3-8B", "TinyLlama-1.1B")
print(f"p-value: {result['wilcoxon_p_value']}")
```

### Custom Visualization:
```python
from visualizations import BenchmarkVisualizer

viz = BenchmarkVisualizer(results_json="results/benchmark_results.json")
viz.plot_overall_comparison("figures/comparison.png")
viz.plot_pass_rate_by_category("figures/by_category.png")
```

## ğŸ”¬ Extending the Framework

### Adding New Tasks

1. Create reference Verilog and testbench files
2. Add entry to `dataset/tasks.json`:
```json
{
  "task_id": "new_task_001",
  "category": "combinational",
  "difficulty": "medium",
  "specification": "Design a...",
  "reference_hdl": "path/to/reference.v",
  "reference_tb": "path/to/testbench.v",
  "inputs": ["a", "b"],
  "outputs": ["y"]
}
```

### Adding New Models

```python
# For Ollama models
model = OllamaInterface("model-name")

# For HuggingFace models
from model_interface import HuggingFaceInterface
model = HuggingFaceInterface("org/model-name")
```

## ğŸ“Š Current Dataset

- âœ… **5 starter tasks** (3 combinational, 2 sequential)
- ğŸš§ **Expanding to 120 tasks** (in progress)
  - 40 combinational circuits
  - 40 sequential circuits
  - 20 FSM designs
  - 20 mixed designs

## ğŸ› ï¸ Development Roadmap

### Phase 1: Core Implementation âœ…
- [x] Pipeline infrastructure
- [x] Model integration (Ollama/HF)
- [x] Starter dataset (5 tasks)
- [x] Statistical analysis
- [x] Visualization module

### Phase 2: Dataset Expansion ğŸš§
- [ ] Expand to 30 tasks (Week 1-2)
- [ ] Expand to 60 tasks (Week 3-4)
- [ ] Reach 120 tasks target (Week 5-6)

### Phase 3: Advanced Features ğŸ“‹
- [ ] Fault injection for testbench evaluation
- [ ] Prompt template variations
- [ ] Coverage analysis
- [ ] Error taxonomy

### Phase 4: Full Benchmark ğŸ“‹
- [ ] Run complete experiments
- [ ] Generate publication-ready results
- [ ] Write research paper

## ğŸ“š Citation

```bibtex
@inproceedings{benchmark-genai-eda,
  title={Benchmarking Generative AI in EDA Workflows},
  author={[Your Name]},
  year={2025},
  note={In preparation}
}
```

## ğŸ“„ License

[Your License Choice]

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“§ Contact

[Your contact information]

## ğŸ™ Acknowledgments

- HDLBits for circuit examples
- OpenCores for reference designs
- Open-source EDA tool developers

---

**Status**: Active Development | **Last Updated**: October 2025

